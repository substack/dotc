{
  "name": "c-tokenizer",
  "version": "0.0.0",
  "description": "tokenize c/c++ source code",
  "main": "index.js",
  "dependencies": {
    "tokenizer": "~1.1.2"
  },
  "devDependencies": {
    "tap": "~0.4.4"
  },
  "scripts": {
    "test": "tap test/*.js"
  },
  "repository": {
    "type": "git",
    "url": "git://github.com/substack/c-tokenizer.git"
  },
  "homepage": "https://github.com/substack/c-tokenizer",
  "keywords": [
    "c",
    "c++",
    "tokenize",
    "lexer",
    "source"
  ],
  "author": {
    "name": "James Halliday",
    "email": "mail@substack.net",
    "url": "http://substack.net"
  },
  "license": "MIT",
  "readme": "# c-tokenizer\n\n[tokenize](https://npmjs.org/package/tokenize) C/C++ source code\n\n[![build status](https://secure.travis-ci.org/substack/c-tokenizer.png)](http://travis-ci.org/substack/c-tokenizer)\n\n# example\n\n``` js\nvar tokenize = require('tokenize');\nvar t = tokenize(function (src, token) {\n    console.log(token.type + ' => ' + JSON.stringify(src));\n});\nprocess.stdin.pipe(t);\n```\n\nFor the input file main.c:\n\n``` c\n#include \"stdio.h\"\n#include \"stdlib.h\"\n\nint main(int argc, char **argv) {\n    printf(\"%d\\n\", foo(atoi(argv[1])));\n    return 0;\n}\n```\n\noutput:\n\n```\n$ node example/tokens.js < example/main.c\ndirective => \"#include\"\nwhitespace => \" \"\nquote => \"\\\"stdio.h\\\"\"\nwhitespace => \"\\n\"\ndirective => \"#include\"\nwhitespace => \" \"\nquote => \"\\\"stdlib.h\\\"\"\nwhitespace => \"\\n\\n\"\nidentifier => \"int\"\nwhitespace => \" \"\nidentifier => \"main\"\nopen paren => \"(\"\nidentifier => \"int\"\nwhitespace => \" \"\nidentifier => \"argc\"\noperator => \",\"\nwhitespace => \" \"\nidentifier => \"char\"\nwhitespace => \" \"\noperator => \"**\"\nidentifier => \"argv\"\nclose paren => \")\"\nwhitespace => \" \"\nopen curly => \"{\"\nwhitespace => \"\\n    \"\nidentifier => \"printf\"\nopen paren => \"(\"\nquote => \"\\\"%d\\\\n\\\"\"\noperator => \",\"\nwhitespace => \" \"\nidentifier => \"foo\"\nopen paren => \"(\"\nidentifier => \"atoi\"\nopen paren => \"(\"\nidentifier => \"argv\"\nopen square => \"[\"\nnumber => \"1\"\nclose square => \"]\"\nclose paren => \")\"\nclose paren => \")\"\nclose paren => \")\"\noperator => \";\"\nwhitespace => \"\\n    \"\nidentifier => \"return\"\nwhitespace => \" \"\nnumber => \"0\"\noperator => \";\"\nwhitespace => \"\\n\"\nclose curly => \"}\"\nwhitespace => \"\\n\"\n```\n\n## var t = tokenize(cb)\n\nReturn a new [tokenize](https://npmjs.org/package/tokenize)\nthrough stream with C/C++ syntax rules loaded into it.\n\nEach parsed token will fire the `cb(src, token)` callback.\n\nEach token has a `token.type` with the rule as a string name and `token.regex`\nas the regular expression for the rule that matched.\n\n## t.addRule(regex, name)\n\nAdd additional rules as `regex` with a `name`.\n\n# install\n\nWith [npm](https://npmjs.org) do:\n\n```\nnpm install c-tokenizer\n```\n\n# license\n\nMIT\n",
  "readmeFilename": "readme.markdown",
  "bugs": {
    "url": "https://github.com/substack/c-tokenizer/issues"
  },
  "_id": "c-tokenizer@0.0.0",
  "_from": "c-tokenizer@~0.0.0"
}
